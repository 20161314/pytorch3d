{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multitexture OBJ Support in PyTorch3d\n",
    "\n",
    "- Support for multitextured OBJs is an important capability for applied deep learning on 3d meshes and point clouds.\n",
    "- A key usecase is applying point cloud segmentation to mesh segmentation problems or embedding labels into obj meshes as outputs of a deep learning pipeline.\n",
    "- This notebook introduces feature enhancements to PyTorch3D that provides multitexture OBJ support for saving, reading, and manipulating OBJs with multiple textures.\n",
    "\n",
    "## Summary of new or ehanced functions\n",
    "1. pytorch3d.ops.sample_points_from_obj() is a new function that allows a user to sample at least one point from all faces with a new auto sampling feature that determines a number of points to sample. Although a new function, sample_points_from_obj repackages existing PyTorch3D functionality from pytorch3d.ops.sample_points_from_meshes(). The enhancements in sample_points_from_obj importantly allow for sampling all faces with a minimum sampling factor and point to face index mappers tensor that allows a link each point to its origin face.\n",
    "2. pytorch3d.ops.sample_points_from_meshes() is modified to enable sample_points_from_obj() by grouping key capabilities into helper functions that can be leveraged in sample_points_from_obj. Further, sample_points_from_meshes is modified slightly to optionally return point-to-face index mappers which can allow a user to recover the face for each point; however, modifications to sample_points_from_meshes are kept minimal and do not provide the new features in sample_points_from_obj.\n",
    "3. pytorch3d.io.obj_io.subset_obj() is a new function that allows a user to subset an obj mesh based on selected face indices. For example, if a workflow predicts a per-face classification, this function can be used to subset the mesh for only those faces.Â \n",
    "4. pytorch3d.io.obj_io.save_obj() and pytorch3d.io.obj_io.load_obj_as_meshes() provide integrated multi-texture obj support to allow users to read and process all available textures; PyTorch3D previously only reads the first texture in an input obj file with multiple textures which can lead to undesirable texture sampling and output.\n",
    "5. pytorch3d.utils.obj_utils provides common utilities for use in both pytorch3d.ops and pytorch3d.io.obj_io such as consolidating obj validation (_validate_obj) and core implementation for subsetting obj data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Install and Import modules\n",
    "Ensure `torch` and `torchvision` are installed. If `pytorch3d` is not installed, install it using the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "# install the pytorch3d fork\n",
    "!pip install 'git+https://github.com/ArcGIS/pytorch3d.git@multitexture-obj-point-sampler'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import open source tools\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from collections import Counter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# import pytorch3d tools\n",
    "from pytorch3d.structures import Pointclouds, join_meshes_as_scene\n",
    "from pytorch3d.ops import  sample_points_from_obj # a new function\n",
    "from pytorch3d.io import (\n",
    "    load_obj,\n",
    "    subset_obj, # a new function\n",
    "    save_obj,\n",
    "    load_objs_as_meshes\n",
    "    )\n",
    "\n",
    "from pytorch3d.renderer import (\n",
    "    look_at_view_transform,\n",
    "    FoVOrthographicCameras, \n",
    "    PointsRasterizationSettings,\n",
    "    PointsRenderer,\n",
    "    PointsRasterizer,\n",
    "    AlphaCompositor,\n",
    "    RasterizationSettings,\n",
    "    MeshRenderer,\n",
    "    MeshRasterizer,\n",
    "    SoftPhongShader,\n",
    "    FoVPerspectiveCameras,\n",
    "    PointLights\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download and make directories for IO or create as needed in local directories\n",
    "!mkdir -p data/cow_mesh\n",
    "!mkdir -p data/output\n",
    "!wget -P data/cow_mesh https://dl.fbaipublicfiles.com/pytorch3d/data/cow_mesh/cow.obj\n",
    "!wget -P data/cow_mesh https://dl.fbaipublicfiles.com/pytorch3d/data/cow_mesh/cow.mtl\n",
    "!wget -P data/cow_mesh https://dl.fbaipublicfiles.com/pytorch3d/data/cow_mesh/cow_texture.png\n",
    "\n",
    "DATA_DIR = os.path.join('data/cow_mesh')\n",
    "OUTPUT_DIR = os.path.join('data/output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    torch.cuda.set_device(device)\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load OBJ and Use sample_points_from_obj\n",
    "\n",
    "The following cells read and write an OBJ, the cow mesh.\n",
    "It renders the cow mesh as in other tutorials except it ensures that at least one point is sampled from each face.\n",
    "1. Since all faces are represented, we can apply point cloud classification or segmentatio to the mesh.\n",
    "2. sample_points_from_obj leverages modifications in the current sample_points_from_meshes \n",
    "3. However, unlike sample_points_from_meshes, sample_points_from_obj only samples for one obj at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the input obj; in this case the tuturial data's cow mesh\n",
    "f = os.path.join(DATA_DIR, 'cow.obj')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the obj into memory\n",
    "obj = load_obj(\n",
    "    f=f,\n",
    "    texture_wrap=None,\n",
    "    create_texture_atlas=True,\n",
    "    texture_atlas_size=8, \n",
    "    device=device\n",
    ")\n",
    "\n",
    "# use sampler to return points from the obj using a specified number of points\n",
    "points, normals, textures, mappers = sample_points_from_obj(\n",
    "    verts=obj[0],\n",
    "    faces=obj[1].verts_idx,\n",
    "    verts_uvs=obj[2].verts_uvs,\n",
    "    faces_uvs=obj[1].textures_idx,\n",
    "    texture_images=obj[2].texture_images,\n",
    "    materials_idx=obj[1].materials_idx,\n",
    "    texture_atlas=obj[2].texture_atlas,\n",
    "    num_samples=None,\n",
    "    min_sampling_factor=100,\n",
    "    sample_all_faces=True,\n",
    "    return_mappers=True, \n",
    "    return_textures=True, \n",
    "    return_normals=True,\n",
    "    use_texture_atlas=True\n",
    ")\n",
    "\n",
    "# squeeze batches out of all tensors\n",
    "(points, normals, textures, mappers) = (\n",
    "    points.squeeze(0),\n",
    "    normals.squeeze(0),\n",
    "    textures.squeeze(0),\n",
    "    mappers.squeeze(0)\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Render the cow mesh as provided by PyTorch3d Tutorials\n",
    "\n",
    "A key new feature is the ability to set num_samples to None and provide a scalar value as a point sampling factor to provide relatively larger or smaller point cloud densities during sample_points_from_obj. In this case, a min_sampling_factor of 100 (very dense) is provided. This feature can be helpful if it is not clear exactly how many points should be sampled to produce a point cloud with sufficient density for deep learning. Specifically, auto sampling produces samples proportionally to face area while considering the number of faces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up a point cloud renderer according to PyTorch3D tutorials\n",
    "# https://pytorch3d.org/tutorials/render_colored_points\n",
    "R, T = look_at_view_transform(2.7, 0, 180)\n",
    "cameras = FoVOrthographicCameras(\n",
    "    device=device,\n",
    "    R=R,\n",
    "    T=T,\n",
    "    znear=0.01\n",
    ")\n",
    "raster_settings = PointsRasterizationSettings(\n",
    "    image_size=256, \n",
    "    radius = 0.003,\n",
    "    points_per_pixel = 10\n",
    ")\n",
    "rasterizer = PointsRasterizer(\n",
    "    cameras=cameras,\n",
    "    raster_settings=raster_settings\n",
    ")\n",
    "renderer = PointsRenderer(\n",
    "    rasterizer=rasterizer,\n",
    "    compositor=AlphaCompositor((128, 128, 128))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "point_cloud = Pointclouds(\n",
    "    points=[points],\n",
    "    features=[textures],\n",
    "    normals=[normals])\n",
    "images = renderer(point_cloud)\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(images[0, ..., :3].cpu().numpy())\n",
    "plt.title(\"A rendered 3D Cow Mesh as a Point Cloud with PyTorch3D\\nSampled with at least one point per face\")\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Apply a Point Cloud Part Segmentation Workflow\n",
    "\n",
    "In this toy example, we use KMeans as a stand-in for a deep learning based point cloud segmentation workflow. Importantly, we can demonstrate how a mesh segmentation problem can be re-framed as a point cloud segmentation problem, if desired. With the new features, this type of re-framing is possible since all faces are represented in the sample. Importantly, this feature can allow researchers to compare and contrast methods for classification and segmentation that apply point cloud or mesh based workflows on the same input meshes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this example we use KMeans clustering as a simplistic way to segment this mean\n",
    "n_clusters = 3\n",
    "\n",
    "scaler = StandardScaler()\n",
    "points_scaled = scaler.fit_transform(points.cpu().numpy())\n",
    "\n",
    "model = KMeans(n_clusters=n_clusters)\n",
    "model.fit(points_scaled)\n",
    "clusters = model.labels_\n",
    "\n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.axis('off')\n",
    "ax.scatter(\n",
    "    points_scaled[..., 2], points_scaled[..., 0], points_scaled[..., 1],\n",
    "    c=clusters)\n",
    "plt.title(f'Finding {n_clusters}x Clusters on a 3D Cow Mesh with KMeans\\nAs a Pseudo Point Class Prediction')\n",
    "plt.show()\n",
    "\n",
    "# the KMeans clusters as predictions per point could be replaced by a better Deep Learning approach\n",
    "assert points.shape[0] == clusters.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# at this point we have a predicted class per point and can apply those classifications to the origin mesh\n",
    "def majority_vote(x: np.ndarray) -> int:\n",
    "    \"\"\"A helper function to count and return the most occuring value in an array.\"\"\"\n",
    "    if x.shape[0] == 1:\n",
    "        return x[0]\n",
    "    else:\n",
    "        if not np.any(x):\n",
    "            return np.bincount(x).argmax()\n",
    "        else:\n",
    "            # for data structs containing negative values\n",
    "            return Counter(x).most_common(1)[0][0]\n",
    "\n",
    "face_classes = np.zeros(obj[1].verts_idx.shape[0])\n",
    "_mappers = mappers.cpu().numpy()\n",
    "\n",
    "for i in range(obj[1].verts_idx.shape[0]):\n",
    "    mask = _mappers == i\n",
    "    face_classes[i] = majority_vote(clusters[mask])\n",
    "\n",
    "print(f'There are {obj[1].verts_idx.shape[0]} faces in the mesh and {np.unique(face_classes).shape[0]} classes')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Produce output OBJ meshes after segmentation\n",
    "\n",
    "We can now combine subset_obj() and save_obj() to produce useful outputs after segmenting a mesh. In this case, we might be interested in writing a subsesh for each part. Optionally we can decide to re-use materials and have each subset reference the same material files to avoid writing the same images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# given a new class or texture for each face, we can apply classes to the mesh\n",
    "unique_classes = np.unique(face_classes).astype(int)\n",
    "subset_obj_files = []\n",
    "\n",
    "# for each face class, return a subset obj of only those faces\n",
    "for unique_class in unique_classes:\n",
    "    # return the faces that belong to the current class as an index\n",
    "    faces_to_subset = np.flatnonzero(face_classes == unique_class)\n",
    "    faces_to_subset = torch.from_numpy(faces_to_subset).to(device)\n",
    "    \n",
    "    # apply the face indices to subset the obj\n",
    "    obj_subset = subset_obj(\n",
    "        obj=obj,\n",
    "        faces_to_subset=faces_to_subset,\n",
    "        device=torch.device('cpu')\n",
    "    )\n",
    "\n",
    "    image_name_kwargs = dict(\n",
    "        reuse_material_files=False # If true will reuse the same material file for subsequet objs\n",
    "    )\n",
    "\n",
    "    obj_name = f\"cow_subset_{unique_class}\"\n",
    "    obj_f = os.path.join(OUTPUT_DIR, f\"{obj_name}.obj\")\n",
    "    texture_images = {}\n",
    "\n",
    "    for k, v in obj_subset[2].texture_images.items():\n",
    "        random_color = np.random.uniform(0, 1, 3)\n",
    "\n",
    "        texture_images[f'{k}_class_{unique_class}'] = torch.from_numpy(np.full(v.shape, random_color))\n",
    "    # save each of the obj subsets as individual objs\n",
    "    save_obj(\n",
    "        f=obj_f,\n",
    "        verts=obj_subset[0],\n",
    "        faces=obj_subset[1].verts_idx,\n",
    "        verts_uvs=obj_subset[2].verts_uvs,\n",
    "        faces_uvs=obj_subset[1].textures_idx,\n",
    "        texture_images=texture_images,\n",
    "        materials_idx=obj_subset[1].materials_idx,\n",
    "        image_name_kwargs=image_name_kwargs,\n",
    "    )\n",
    "    # print(obj_subset[2].texture_images)\n",
    "    subset_obj_files.append(obj_f)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Read and validate multitexture OBJ support\n",
    "\n",
    "To validate the results, we can also use load_objs_as_meshes with multitexture OBJ support to read in the previous outputs as a single mesh. Notice that the result is the same as the prior output; this means we were able to deconstruct and reconstruct the OBJ mesh by semantic segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up rendering for a meshes object according to PyTorch3d Tutorials\n",
    "# https://pytorch3d.org/tutorials/fit_textured_mesh\n",
    "R, T = look_at_view_transform(2.7, 0, 180)\n",
    "cameras = FoVPerspectiveCameras(device=device, R=R, T=T)\n",
    "\n",
    "raster_settings = RasterizationSettings(\n",
    "    image_size=256,\n",
    "    blur_radius=0.0,\n",
    "    faces_per_pixel=1,\n",
    ")\n",
    "\n",
    "lights = PointLights(device=device, location=[[0.0, 0.0, -3.0]])\n",
    "\n",
    "renderer = MeshRenderer(\n",
    "    rasterizer=MeshRasterizer(cameras=cameras, raster_settings=raster_settings),\n",
    "    shader=SoftPhongShader(device=device, cameras=cameras, lights=lights),\n",
    ")\n",
    "\n",
    "# read in each of the subsets as a batch of individual meshes\n",
    "mesh_from_subsets = load_objs_as_meshes(\n",
    "    files=subset_obj_files,\n",
    "    texture_wrap=None,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# join the input meshes as a scene\n",
    "mesh = join_meshes_as_scene(mesh_from_subsets)\n",
    "# render the image: we should have the input cow\n",
    "images = renderer(mesh)\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(images[0, ..., :3].cpu().numpy())\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.title(\"A 3D Cow Mesh Reconstructed From Subsets\\nColored by Predicted Class of Points Sampled from Each Face\")\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Write OBJ with Multiple Textures\n",
    "\n",
    "Finally, a key new feature can allow users to write to disc a single OBJ with multiple textures and with full or high precision vertices. In this case, each material is represented by a random color meant to visualize classes in the mesh. In this example, we use KMeans to provide an arbitrary classification for each face via their sampled points; however, in complex cases, KMeans could be replaced with a state of the art model that produces a per-point classification or segmentation result. \n",
    "\n",
    "In the following cells, we manually merge an OBJ mesh so that the original cow mesh, which had only one material/texture, now has at least three materials (one for each cluster or class). Then, we exercise the new feature of save_obj to save this obj to disc and read it back to provide the expected mesh. Without this new feature, it is only possible to read the first material in a list of materials. For outputs, a user *can* write a Meshes object with multiple textures as a single concatenated scene of materials; however, this method does not generalize well. The reason for this is that if there are many large textures, as is the case for city or regional meshes, PyTorch3D will run into memory IO limitations in writing single, large images to disc."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.a. Working with multitexture OBJ IO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can leverage multitexture obj support for saving meshes to disc with multiple textures\n",
    "\n",
    "# read in the obj from disc into memory subset objs by class but accumulate them into a single obj file\n",
    "offset = 0\n",
    "verts, faces, verts_uvs, faces_uvs, materials_idx = [], [], [], [], []\n",
    "texture_images = {}\n",
    "material_counter = 0\n",
    "\n",
    "for f in subset_obj_files:\n",
    "    obj_subset = load_obj(\n",
    "        f=f,\n",
    "        texture_wrap=None, \n",
    "    )\n",
    "    # accumulate and offset tensors \n",
    "    verts.append(obj_subset[0])\n",
    "    faces.append(obj_subset[1].verts_idx + offset)\n",
    "    verts_uvs.append(obj_subset[2].verts_uvs)\n",
    "    faces_uvs.append(obj_subset[1].textures_idx)\n",
    "    \n",
    "    for k, v in obj_subset[2].texture_images.items():\n",
    "        # this example is a special case where we can simply accumulate textures\n",
    "        texture_images[k] = v\n",
    "        # in a general case, we need to handle seeing the same texture more than once\n",
    "            \n",
    "    materials_idx.append(obj_subset[1].materials_idx + material_counter)\n",
    "    offset += obj_subset[0].shape[0]\n",
    "    # in the general case, we would need to adjust how materials_idx are offset\n",
    "    material_counter += 1\n",
    "\n",
    "obj_name = f\"cow_multitexture_subset\"\n",
    "obj_f = os.path.join(OUTPUT_DIR, f\"{obj_name}.obj\")\n",
    "\n",
    "# save the resulting obj with multiple textures into a mesh\n",
    "save_obj(\n",
    "    f=obj_f,\n",
    "    verts=torch.cat(verts),\n",
    "    faces=torch.cat(faces),\n",
    "    verts_uvs=torch.cat(verts_uvs),\n",
    "    faces_uvs=torch.cat(faces_uvs),\n",
    "    texture_images=texture_images,\n",
    "    materials_idx=torch.cat(materials_idx),\n",
    "    image_name_kwargs=image_name_kwargs,\n",
    ")\n",
    "\n",
    "\n",
    "mesh = load_objs_as_meshes(\n",
    "    files=[obj_f],\n",
    "    device=device\n",
    ")\n",
    "images = renderer(mesh)\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(images[0, ..., :3].cpu().numpy())\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.title(\"A 3D Cow Mesh With Multitexture IO with OBJs and Meshes\")\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.b. Working with Real World Coordinate Geometries with High Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the same obj output from the preceding cells, test high precision\n",
    "obj = load_obj(\n",
    "    f=obj_f,\n",
    ")\n",
    "\n",
    "# we can simulate the case of real-world coordinate geometries by adding a large value to x y and z\n",
    "verts_fp32 = obj[0] + 5000000\n",
    "print(verts_fp32)\n",
    "# as we can see from the print out, float32 starts rounding values which might represent real coordinates\n",
    "# this effect compounds if sampling from this mesh since the points also lack precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the same obj output from the preceding cells, test high precision\n",
    "obj = load_obj(\n",
    "    f=obj_f,\n",
    "    high_precision=True\n",
    ")\n",
    "\n",
    "# we can simulate the case of real-world coordinate geometries by adding a large value to x y and z\n",
    "verts_fp64 = obj[0] + 5000000\n",
    "print(verts_fp64)\n",
    "# in this case, since the tensor can handle full precision values, the data is not rounded\n",
    "# for high precision verts, the resulting sampled points produce ideal coordinates"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In Summary:\n",
    "\n",
    "- This notebook demonstrates multitexture OBJ IO support and enhanced point cloud sampling with PyTorch3D\n",
    "- IO for multitextured OBJs is a new feature in this fork of PyTorch3d.\n",
    "- A key use case is applying point cloud segmentation to mesh segmentation problems.\n",
    "- *sample_points_from_obj()* allows for sampling at least one point from all faces with auto sampling to determine number of points to sample in addition to providing an index of mappers to recover each point's face origin.\n",
    "- *sample_points_from_meshes()* is modified to allow code-reuse for sample_points_from_obj() and to provide mappers.\n",
    "- KMeans clustering is used here to represent a point cloud segmentation result but could be replaced by a state of the art point segmentation model.\n",
    "- The point classifications from KMeans are applied to the origin faces in the mesh as obj subsets with *subset_obj()* and saved as both individual obj meshes and composite meshes with multiple textures.\n",
    "- The results of each input and output is rendered using PyTorch3d tutorial methods for point clouds and meshes and demonstrates how modifications to *save_obj()* and *load_obj()* provide integrated multitexture obj support.\n",
    "- Lastly, we provide a feature to support obj IO with float64 or *high_precision* which allows for flexiblity in workin with real world coordinate geometries such as those found in building and city-scape meshes.\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "bento_stylesheets": {
   "bento/extensions/flow/main.css": true,
   "bento/extensions/kernel_selector/main.css": true,
   "bento/extensions/kernel_ui/main.css": true,
   "bento/extensions/new_kernel/main.css": true,
   "bento/extensions/system_usage/main.css": true,
   "bento/extensions/theme/main.css": true
  },
  "colab": {
   "name": "bundle_adjustment.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
